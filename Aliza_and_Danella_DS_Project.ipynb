{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asusatijo/DS2002-Data-Project-1/blob/main/Aliza_and_Danella_DS_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DS2002 Data Project\n",
        "\n",
        "Group Names and Computing IDs\n",
        "\n",
        "Danella Lei Romera (tzb5xh) and Aliza Susatijo (egg2qp)"
      ],
      "metadata": {
        "id": "JGDDBRj-QpGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import sqlite3\n",
        "import json\n",
        "import io\n",
        "\n",
        "\n",
        "# Function to fetch data from an API URL\n",
        "def fetch_data_from_api(api_url):\n",
        "    response = requests.get(api_url)\n",
        "    response.raise_for_status()  # Raise an error for HTTP issues\n",
        "    return response.content\n",
        "\n",
        "\n",
        "#Converts the data fetched from the api url into a dataframe\n",
        "def convert_and_process_data(input_format, data):\n",
        "\n",
        "\n",
        "    #checks whether the input format is csv or json\n",
        "    if input_format == 'csv':\n",
        "        return pd.read_csv(io.StringIO(data.decode('utf-8')))\n",
        "    elif input_format == 'json':\n",
        "        return pd.json_normalize(json.loads(data.decode('utf-8')))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid Input: Input format must be csv or json\")\n",
        "\n",
        "\n",
        "# Modifies the dataframe based on what columns the user wants to keep and add. Then it saves the data to the output format specified\n",
        "def modify_and_save_data(df, output_format, output_path=None, db_name=None, table_name=None, columns_to_keep=None, new_columns=None):\n",
        "\n",
        "\n",
        "    #Prints a summary of ingestion data\n",
        "    print(f\"Ingestion Summary: Records = {len(df)}, Columns = {df.shape[1]}\")\n",
        "    print(df.columns)\n",
        "\n",
        "\n",
        "    #Modifies what columns the dataframe keeps\n",
        "    if columns_to_keep:\n",
        "        df = df[columns_to_keep]  # Keep specified columns\n",
        "\n",
        "\n",
        "    #Adds new columns\n",
        "    if new_columns:\n",
        "        for column_name, value in new_columns.items():\n",
        "            df[column_name] = value\n",
        "\n",
        "\n",
        "    # Saves data to the specified output format\n",
        "    if output_format == 'csv':\n",
        "        if output_path:\n",
        "            df.to_csv(output_path, index=False)\n",
        "            print(f\"Data written to CSV at {output_path}\")\n",
        "        else:\n",
        "            print(df.to_csv(index=False))\n",
        "\n",
        "\n",
        "    elif output_format == 'json':\n",
        "        if output_path:\n",
        "            df.to_json(output_path, orient=\"records\")\n",
        "            print(f\"Data written to JSON at {output_path}\")\n",
        "        else:\n",
        "            df.to_json(\"project1.txt\", orient=\"records\")\n",
        "            print(\"Data written to JSON at project1.txt\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        #Raises an error if no sql database name is provided\n",
        "        if db_name:\n",
        "            #connects to the database\n",
        "            conn = sqlite3.connect(db_name)\n",
        "\n",
        "\n",
        "            # Convert unsupported data types to strings before insertion\n",
        "            for column in df.columns:\n",
        "                if df[column].dtype not in [str, int, float]:  # Check if dtype is supported\n",
        "                    df[column] = df[column].astype(str)  # Convert to string\n",
        "            #saves dataframe to sql. Will use default table name if none is provided\n",
        "            if table_name:\n",
        "                df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "            else:\n",
        "                df.to_sql(\"default_table\", conn, if_exists='replace', index=False)\n",
        "\n",
        "\n",
        "            conn.close()\n",
        "            print(f\"Data stored in SQL database: {db_name}, table: {table_name}\")\n",
        "        else:\n",
        "          raise ValueError(\"Invalid Input: Make sure you enter in the database name\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Summary of post-processing\n",
        "    print(f\"Post-processing Summary: Records = {len(df)}, Columns = {df.shape[1]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Asks the user and returns what input format, output format, columns of the data from the api they want. The user can also add new columns and data to the data frame\n",
        "def interactive_etl_process(api_url):\n",
        "    #Gets the input and output format from the user\n",
        "    input_format = input(\"Enter input format (csv, json): \").lower()\n",
        "    output_format = input(\"Enter output format (csv, json, sql): \").lower()\n",
        "\n",
        "\n",
        "    #retrives the data from the api\n",
        "    data = fetch_data_from_api(api_url)\n",
        "\n",
        "\n",
        "    #converts the data into a dataframe\n",
        "    df = convert_and_process_data(input_format, data)\n",
        "\n",
        "\n",
        "    #prints out the columns of the DataFrame so the user can choose what columns to keep\n",
        "    print(f\"Columns in Table:  {df.columns.to_list()}\")\n",
        "\n",
        "\n",
        "    #Gets the columns the user wants from the data frame\n",
        "    columns_to_keep = input(\"Enter columns to keep (comma-separated, or leave blank for all): \")\n",
        "    if columns_to_keep:\n",
        "        columns_to_keep = [col.strip() for col in columns_to_keep.split(',')]\n",
        "    else:\n",
        "        coloumns_to_keep = None\n",
        "\n",
        "\n",
        "    #Gets the new columns and values the user wants to add to the data frame\n",
        "    new_columns_input = input(\"Enter new columns to add in the format 'column_name:value' (comma-separated, or leave blank for none): \")\n",
        "    new_columns = {}\n",
        "    if new_columns_input:\n",
        "        for item in new_columns_input.split(','):\n",
        "            #makes sure input is in correct \"column_name:value\" format\n",
        "            parts = item.split(':')\n",
        "            if len(parts) != 2:\n",
        "                raise ValueError(\"Input must be in the format 'column_name:value'.\")\n",
        "            column_name, value = parts\n",
        "            new_columns[column_name.strip()] = value.strip()\n",
        "\n",
        "\n",
        "    #Saves the data to the users specified database name and table name if the output format is sql\n",
        "    #Saves the data to the output file path the user entered in if the output format is csv or json\n",
        "    if output_format == 'sql':\n",
        "        db_name = input(\"Enter the SQL database name (add .sql at the end): \")\n",
        "        table_name = input(\"Enter name for table: \")\n",
        "        modify_and_save_data(df, output_format, db_name=db_name, table_name=table_name, columns_to_keep=columns_to_keep, new_columns=new_columns)\n",
        "    elif output_format == 'json' or output_format == 'csv':\n",
        "        output_path = input(\"Enter the output file path (add .txt at the end): \")\n",
        "        modify_and_save_data(df, output_format, output_path=output_path, columns_to_keep=columns_to_keep, new_columns=new_columns)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid Input: Output format must be csv, json, or sql\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#processes data for the given api url\n",
        "def main():\n",
        "    api_URL = 'https://random-d.uk/api?format=json'\n",
        "    interactive_etl_process(api_URL)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gLYL4k-CdOT",
        "outputId": "85268b81-d8d2-4463-8ff5-5d882e0d2ffe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter input format (csv, json): json\n",
            "Enter output format (csv, json, sql): sql\n",
            "Columns in Table:  ['Base URL', 'Upload', 'v1.GET', 'v2.GET']\n",
            "Enter columns to keep (comma-separated, or leave blank for all): \n",
            "Enter new columns to add in the format 'column_name:value' (comma-separated, or leave blank for none): \n",
            "Enter the SQL database name (add .sql at the end): file.sql\n",
            "Enter name for table: duck\n",
            "Ingestion Summary: Records = 1, Columns = 4\n",
            "Index(['Base URL', 'Upload', 'v1.GET', 'v2.GET'], dtype='object')\n",
            "Data stored in SQL database: file.sql, table: duck\n",
            "Post-processing Summary: Records = 1, Columns = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uykigO8hCwcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}